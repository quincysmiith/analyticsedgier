install.packages("mime")
Sys.getenv("R_LIBS_USER")
install.packages("mime")
install.packages("mime")
install.packages("knitr")
install.packages("forecast")
install.packages("data.table")
install.package("slidify")
install.packages("slidify")
install.packages("devtools")
source("http://bioconductor.org/biocLite.R")
biocLite()
install.packages("ggplot2")
install.packages(KernSmooth)
install.packages("KernSmooth")
library(KErnSmooth)
library(KernSmooth)
library("KernSmooth")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("devtools")
install.packages("ggplot2")
install.packages("dplyr")
install.packages("ggplot2")
library("ggplot2", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.0")
install.packages("shiny")
install.packages("tm")
install.packages("SnowballC")
install.packages("flexclust")
install.packages("tm")
install.packages("tm")
install.packages("SnowballC")
install.packages("flexclust")
install.packages("ggplot2")
install.packages("maps")
install.packages("ggmap")
install.packages("igraph")
install.packages("wordcloud")
install.packages("RColorBrewer")
x <- 1
print(x)
x
cube <- function(x,n){
x^3
}
cube(3)
library(datasets)
data(iris)
?iris
type(iris)
dimnames(iris)
library(datasets)
data(iris)
data <- dimnames(iris)
library(datasets)
data(iris)
?iris
head(iris)
str(iris)
lapply(s,mean, s$Sepal.Length)
s <- split(iris, iris$Species)
lapply(s,mean, s$Sepal.Length)
lapply(s, function(x) colMeans(x[, c("Sepal.Length", "Sepal.Width")]))
lapply(s, function(x) mean(x[, c("Sepal.Length", "Sepal.Width")]))
apply(iris[, 1:4], 2, mean)
apply(iris, 2, mean)
library(datasets)
data(mtcars)
head(mtcars)
with(mtcars, tapply(mpg, cyl, mean))
with(mtcars, tapply(mpg, gear, mean))
summary(mtcars)
s <- with(mtcars, tapply(mpg, cyl, mean))
s
s[1]
s[1][1]
s[1] - s[2]
s[1] - s[3]
with(mtcars, tapply(mpg, cyl, mean))
with(mtcars, tapply(hp, cyl, mean))
s <- with(mtcars, tapply(hp, cyl, mean))
s[1] - s[3]
?inv
?solve
c(2,3,5,8,13)
Country <- c("Brazil", "China" , "India", "Switzwerland", "USA")
LifeExpectancy <- c(74,76,65,83,79)
Country
LifeExpactancy
LifeExpectancy
Country[1]
seq(0, 100, 2)
CountryData <- data.frame(Country, LifeExpactancy)
CountryData <- data.frame(Country, LifeExpectancy)
CountryData
CountryData$Population <- c(199000, 1390000, 1240000, 7997, 318000)
install.packages("ROCR")
setwd("~/Documents/analyticsedgier/Week 04. Trees/03. Regression Trees for housing data")
boston <- read.csv("boston.csv")
str(boston)
plot(boston$LON, boston$LAT)
# Add pints that lie on boston river
points(boston$LON[boston$CHAS ==1], boston$LAT[boston$CHAS ==1], col="blue", pch=19)
points(boston$LON[boston$TRACT == 3531], boston$LAT[boston$TRACT == 3531], col="red", pch=19)
summary(boston$NOX)
points(boston$LON[boston$NOX > 0.55], boston$LAT[boston$NOX > 0.55], col="green", pch=19)
# create a new plot to investigate prices
plot(boston$LON, boston$LAT)
summary(boston$MEDV)
points(boston$LON[boston$MEDV > 21.2], boston$LAT[boston$MEDV > 21.2], col="red", pch=19)#
plot(boston$LAT, boston$MEDV)
plot(boston$LON, boston$MEDV)
latlonlm <- lm(MEDV ~ LAT + LON, data=boston)
summary(latlonlm)
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV > 21.2], boston$LAT[boston$MEDV > 21.2], col="red", pch=19)
# where does our linear regression model think the above average in price houses are
points(boston$LON[latlonlm$fitted.values >= 21.2], boston$LAT[latlonlm$fitted.values >= 21.2],
col="blue", pch="$")
library(rpart)
library(rpart.plot)
latlontree <- rpart(MEDV ~ LAT + LON, data=boston)
prp(latlontree)
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV > 21.2], boston$LAT[boston$MEDV > 21.2], col="red", pch=19)
fittedvalues <- predict(latlontree)
points(boston$LON[latlonlm$fittedvalues >= 21.2], boston$LAT[latlonlm$fittedvalues >= 21.2],
col="blue", pch="$")
points(boston$LON[fittedvalues >= 21.2], boston$LAT[fittedvalues >= 21.2],
col="blue", pch="$")
?rpart
latlontree2 <- rpart(MEDV ~ LAT + LON, data=boston, minbucket=50)
plot(latlontree2)
text(latlontree2)
plot(boston$LON, boston$LAT)
abline(v=-71.07)
abline(h=42.21)
abline(h=42.17)
points(boston$LON[boston$MEDV > 21.2], boston$LAT[boston$MEDV > 21.2], col="red", pch=19)
library(caTools)
set.seed(123)
split <- sample.split(boston$MEDV, SplitRatio=0.7)
bostonTrain <- subset(boston, split==TRUE)
bostonTest <- subset(boston, split==FALSE)
names(boston)
linreg <- lm(MEDV ~ LAT + LON + CRM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO,
data=bostonTrain)
summary(linreg)
linreg <- lm(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO,
data=bostonTrain)
summary(linreg)
linreg.pred <- predict(linreg, newdata=bostonTest)
linreg.sse <- sum((linreg.pred - bostonTest$MEDV)^2)
linreg.sse
tree <- rpart(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO,
data=bostonTrain)
prp(tree)
tree.predict <- predtct(tree, newdata=bostonTest)
tree.sse <- sum((tree.pred - bostonTest$MEDV)^2)
tree.sse
tree.pred <- predict(tree, newdata=bostonTest)
tree.sse <- sum((tree.pred - bostonTest$MEDV)^2)
tree.sse
library(caret)
library(e1071)
tr.control <- trainControl(method="cv", number = 10)
cp.grid <- expand.grid(.cp=(0:10) * 0.001)
cp.grid
names(boston)
tr <- train(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO,
data = bostonTrain, method="rpart", trControl=tr.control, tuneGrid=cp.grid)
tr
best.tree <- tr$finalModel
prp(best.tree)
setwd("~/Documents/analyticsedgier/Week 04. Trees/Homeworks")
gerber <- read.csv("gerber.csv")
str(gerber)
nrow(subset(gerber, voting==1)) / nrow(gerber)
table(gerber$hawthorne)
nrow(subset(gerber[gerber$civicduty == 1], voting==1)) / nrow(gerber[gerber$civicduty == 1])
gerber_civic <- gerber[gerber$civicduty == 1]
gerber_civic <- gerber[gerber$civicduty == 1,]
nrow(subset(gerber[gerber$civicduty == 1,], voting==1)) / nrow(gerber[gerber$civicduty == 1,])
nrow(subset(gerber[gerber$hawthorne == 1,], voting==1)) / nrow(gerber[gerber$hawthorne == 1,])
nrow(subset(gerber[gerber$self == 1,], voting==1)) / nrow(gerber[gerber$self == 1,])
nrow(subset(gerber[gerber$neighbors == 1,], voting==1)) / nrow(gerber[gerber$neighbors == 1,])
gerberLog <- glm(voting ~ hawthorne + civicduty + neighbors + self,
data = gerber, family="binomial")
summary(gerberLog)
predictLog <- predict(gerberLog, type="response")
table(gerber$voting, predictLog > 0.3)
accuracy <- (134513 + 51966) / nrow(gerber)
accuracy
table(gerber$voting, predictLog > 0.5)
accuracy <- (235388) / nrow(gerber)
accuracy
CARTmodel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber)
prp(CARTmodel)
CARTmodel2 = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0)
prp(CARTmodel2)
CARTmodel3 = rpart(voting ~ civicduty + hawthorne + self + neighbors + sex, data=gerber, cp=0.0)
prp(CARTmodel3)
gerberControl <- subset(gerber, control == 1)
table(gerberControl$sex, gerberControl$voting)
male <- 29015 / (66809 + 29015)
female <- 27715 / (67704 + 27715)
male
female
gerberCivic <- subset(gerber, civicduty == 1)
table(gerberCivic$sex, gerberCivic$voting)
male <- 6165 / (12937 + 6165)
female <- 5856 / (13260 + 5856)
male
female
CARTmodel4 = rpart(voting ~ control, data=gerber, cp=0.0)
prp(CARTmodel4, digits=6)
0.34 - 0.296638
CARTmodel5 = rpart(voting ~ control + sex, data=gerber, cp=0.0)
prp(CARTmodel5, digits=6)
female <- abs(0.290456 -0.334176)
male <- abs(0.302795 - 0.345818)
female
male
female - male
gerberLog2 <- glm(voting ~ control + sex, data=gerber, family="binomial")
summary(gerberLog2)
Possibilities = data.frame(sex=c(0,0,1,1),control=c(0,1,0,1))
predict(gerberLog2, newdata=Possibilities, type="response")
abs(0.290456 - 0.2908065)
LogModel2 = glm(voting ~ sex + control + sex:control, data=gerber, family="binomial")
summary(LogModel2)
predict(LogModel2, newdata=Possibilities, type="response")
abs(0.290456 - 0.2904558)
2e-07
2e-07 + 1
setwd("~/Documents/analyticsedgier/Week 04. Trees/Homeworks")
letters <- read.csv("letters_ABPR.csv")
letters$isB = as.factor(letters$letter == "B")
set.seed(1000)
split <- sample.split(letters$isB, SplitRatio=50)
lettersTrain <- subset(letters, split==TRUE)
lettersTest <- subset(letters, split==FALSE)
set.seed(1000)
split <- sample.split(letters$isB, SplitRatio=0.50)
lettersTrain <- subset(letters, split==TRUE)
lettersTest <- subset(letters, split==FALSE)
table(letters$isB)
2350 / nrow(letters)
CARTb_pred <- predict(CARTb, type="class")
table(letters$letter, CARTb_pred)
CARTb = rpart(isB ~ . - letter, data=train, method="class")
CARTb_pred <- predict(CARTb, type="class")
table(letters$letter, CARTb_pred)
CARTb = rpart(isB ~ . - letter, data=lettersTrain, method="class")
CARTb_pred <- predict(CARTb, type="class")
table(letters$letter, CARTb_pred)
table(lettersTrain$letter, CARTb_pred)
accuracy <- (401 + 347 + 397 + 341) / nrow(lettersTrain)
accuracy
accuracy <- (347 ) / (347 + 36)
accuracy
CARTb_pred <- predict(CARTb, newdata = lettersTest, type="class")
table(lettersTrain$letter, CARTb_pred)
accuracy <- (303 + 102 + 309 + 268 ) / nrow(lettersTest)
accuracy
table(lettersTest$letter, CARTb_pred)
accuracy <- (380 + 340 + 386 + 352 ) / nrow(lettersTest)
accuracy
?randomforest
??randomforest
?randomForest
forestB <- randomForest(isB ~ . -letter, data=lettersTrain)
library("randomForest")
forestB <- randomForest(isB ~ . -letter, data=lettersTrain)
summary(forestB)
set.seed(1000)
forestB <- randomForest(isB ~ . -letter, data=lettersTrain)
#summary(forestB)
forestB_pred <- predict(forestB, newdata=lettersTest)
table(lettersTest$isB, forestB_pred)
accuracy <- (1165 + 374) / nrow(lettersTest)
accuracy
letters$letter = as.factor( letters$letter )
set.seed(2000)
split2 <- sample.split(letters$letter, SplitRatio = 0.5)
lettersTrain2 <- subset(letters, split2 == TRUE)
lettersTest2 <- subset(letters, split2 == FALSE)
table(lettersTest2$letter)
401/ nrow(lettersTest2)
letterTree <- rpart(letter ~ . - isB, data=lettersTrain2, method="class")
letterTree_pred <- predict(letterTree, newdata = lettersTest2, type="class")
table(lettersTest$letter, letterTree_pred)
table(lettersTest2$letter, letterTree_pred)
(348 + 318 + 363 + 340) / nrow(lettersTest2)
letterForest <- randomForest(letter ~ . - isB, data=lettersTrain2)
letterForest_pred <- predict(letterForest, newdata = lettersTest2, type="class")
table(lettersTest2$letter, letterForest_pred)
(390 + 380 + 393 + 369) / nrow(lettersTest2)
setwd("~/Documents/analyticsedgier/Week 04. Trees/Homeworks")
census <- read.csv("census.csv")
census <- read.csv("census.csv")
str(census)
set.seed(2000)
split <- sample.split(census$over50k, SplitRatio=0.6)
censusTrain <- subset(census, split==TRUE)
censusTest <- subset(census, split==FALSE)
censusLog <- glm(over50k ~ ., data=census, family=binomial)
summary(censusLog)
censusLog <- glm(over50k ~ ., data=censusTrain, family=binomial)
summary(censusLog)
censusLog_pred <- predict(censusLog, newdata=censusTest, type="response")
table(censusTrain$over50k, censusLog_pred > 0.5)
table(censusTest$over50k, censusLog_pred > 0.5)
accuracy <- (9051 + 1888) / nrow(censusTest)
accuracy
table(censusTest$over50k)
9713 / nrow(censusTest)
auc <- as.numeric(performance(censusLog_pred, "auc")@y.values)
auc
library("ROCR")
auc <- as.numeric(performance(censusLog_pred, "auc")@y.values)
auc
ROCRpredTest <- prediction(censusLog_pred , censusTest$over50k)
auc <- as.numeric(performance(ROCRpredTest, "auc")@y.values)
auc
censusTree <- rpart(over50k ~ ., data=censusTrain, method="class")
prp(censusTree)
censusTree_pred <- predict(censusTree, newdata=censusTest, type="class")
table(censusTest$over50k, censusTree_pred)
accuracy <- (9243 + 1596) / nrow(censusTest)
accuracy
censusTree_pred2 <- predict(censusTree, newdata=censusTest, type="class")
censusTree_pred2[2,]
censusTree_pred2[,2]
censusTree_pred2 <- predict(censusTree, newdata=censusTest)
censusTree_pred2[,2]
censusTree_pred2[2,2]
censusTree_pred2[2,]
ROCRpred <- prediction(censusTree_pred2[,2], censusTrain$over50k)
ROCRperf <- performance(ROCRpred, "tpr", "fpr")
plot(ROCRperf)
set.seed(1)
censusTrainSmall <-  censusTrain[sample(nrow(censusTrain), 2000), ]
set.seed(1)
censusForest <- randomForest(over50k ~ ., data=censusTrainSmall)
censusForest_pred <- predict(censusForest, newdata=censusTest)
table(censusTest$over50k, censusForest_pred)
accuracy <- (9586 + 1093) / nrow(censusTest)
accuracy
vu <- varUsed(censusForest, count=TRUE)
vusorted <- sort(vu, decreasing = FALSE, index.return = TRUE)
dotchart(vusorted$x, names(MODEL$forest$xlevels[vusorted$ix]))
vu <- varUsed(censusForest, count=TRUE)
vusorted <- sort(vu, decreasing = FALSE, index.return = TRUE)
dotchart(vusorted$x, names(censusForest$forest$xlevels[vusorted$ix]))
varImpPlot(censusForest)
train(over50k ~ ., data=censusTrain, method = "rpart", trControl=numFolds, tuneGrid=cartGrid)
numFolds <- trainControl(method = "cv", number=10)
cartGrid = expand.grid( .cp = seq(0.002,0.1,0.002))
train(over50k ~ ., data=censusTrain, method = "rpart", trControl=numFolds, tuneGrid=cartGrid)
numFolds <- trainControl(method = "cv", number=10)
cartGrid = expand.grid( .cp = seq(0.002,0.1,0.002))
set.seed(2)
train(over50k ~ ., data=censusTrain, method = "rpart", trControl=numFolds, tuneGrid=cartGrid)
censusTreeCV <- rpart(over50k ~ ., data=censusTrain, method="class", cp=0.002)
prp(censusTreeCV)
censusTreeCV_pred <- predict(censusTreeCV, newdata=censusTest)
table(censusTest$over50k, censusTreeCV_pred)
censusTreeCV_pred <- predict(censusTreeCV, newdata=censusTest, type="class")
table(censusTest$over50k, censusTreeCV_pred)
accuracy <- (9178 + 1838) / nrow(censusTest)
accuracy
